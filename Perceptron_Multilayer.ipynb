{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c0a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random                          # crear numeros aleatoriamente\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa13cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324, 3)\n"
     ]
    }
   ],
   "source": [
    "# cargar el archivo .mat\n",
    "data = sio.loadmat('Tiosulfatosodio.mat')\n",
    "\n",
    "#Extrayendo matriz de caracteristicas \n",
    "Xi = data['x']\n",
    "\n",
    "#Extrayendo dato esperado (Clases)\n",
    "Yi = data['y']\n",
    "\n",
    "#Agregando el bias a las filas de la matriz de caracteristicas (Entradas)\n",
    "bias = np.full((Xi.shape[0], 1), 1)\n",
    "Xi = np.hstack((bias, Xi))\n",
    "\n",
    "#numero de datos\n",
    "num_datos = Xi.shape[0]\n",
    "\n",
    "#numero de caracteristicas\n",
    "caract = Xi.shape[1]                  # Se resta el bias \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Definicion de errores\n",
    "Err_entr = 10;\n",
    "Err_pru = 10;\n",
    "\n",
    "#Datos de entrenamiento y prueba: entrenamiento 90% , prueba 10%\n",
    "X_ent, X_pru, Y_ent, Y_pru = train_test_split(Xi, Yi, test_size=0.1)\n",
    "\n",
    "#Obteniendo datos de validacion: Validacion 10% y Entrenamiento 80%\n",
    "#Se extraen los datos de validación a partir de los de entrenamiento\n",
    "X_ent, X_val, Y_ent, Y_val = train_test_split(X_ent, Y_ent, test_size=0.1, random_state=42)\n",
    "\n",
    "#Obteniendo datos de entrenamiento y prueba\n",
    "D_entre  =  len(X_ent)       # datos de entrenamiento   (verificar la cantidad de dtos para hacer la particion)\n",
    "D_prueba =  len(X_pru)       #Datos de validacion \n",
    "D_val    =  len(X_val)\n",
    "\n",
    "print(X_ent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07588702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de funciones\n",
    "\n",
    "def sigmoide(x):\n",
    "    \"\"\"\n",
    "    Función de activación sigmoide\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoide_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivada de la función de activación sigmoide\n",
    "    \"\"\"\n",
    "    return sigmoide(x) * (1 - sigmoide(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2fc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definicion de parametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8783e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb99845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.487624  , 0.82662256, 0.82352165, 0.25655762],\n",
      "       [0.28591668, 0.63172653, 0.68398418, 0.83408014],\n",
      "       [0.19685234, 0.510771  , 0.69497811, 0.69214576]]), array([[0.27629739, 0.42080157, 0.03143996],\n",
      "       [0.03797378, 0.38133657, 0.38460747],\n",
      "       [0.98848373, 0.69589691, 0.21630594],\n",
      "       [0.09295406, 0.64712933, 0.68398627]]), array([[0.36458884, 0.95836685],\n",
      "       [0.48025151, 0.10160117],\n",
      "       [0.0802393 , 0.87580176]]), array([[0.57813152],\n",
      "       [0.78379846]])]\n",
      "Y_pred [[ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]]\n",
      "Y_ent [[-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]]\n",
      "Error:  51.54320987654321\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Función de activación sigmoidal\n",
    "    #return 1 / (1 + np.exp(-x))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Derivada de la función de activación sigmoidal\n",
    "    #return (sigmoid(x) * (1 - sigmoid(x)))\n",
    "    return (1-np.tanh(x)**2)\n",
    "    \n",
    "def perceptron_multicapa(X, y, hidden_layers, hidden_units, lr, epochs):\n",
    "    # X: matriz de características (muestra x características)\n",
    "    # y: vector de etiquetas (muestra x 1)\n",
    "    # hidden_layers: número de capas ocultas\n",
    "    # hidden_units: número de unidades en cada capa oculta (lista de longitud hidden_layers)\n",
    "    # lr: tasa de aprendizaje\n",
    "    # epochs: número de iteraciones sobre el conjunto de entrenamiento\n",
    "\n",
    "    # Inicialización aleatoria de los pesos\n",
    "    input_units = X.shape[1]\n",
    "    output_units = 1\n",
    "    layers_units = [input_units] + hidden_units + [output_units]\n",
    "    #weights = [np.random.rand(layers_units[i], layers_units[i+1]) for i in range(hidden_layers+1)]\n",
    "    weights = [np.random.rand(layers_units[i], layers_units[i+1]) for i in range(hidden_layers+1)]\n",
    "    print(weights)\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        # Propagación hacia adelante\n",
    "        layer_outputs = [X]\n",
    "        for i in range(hidden_layers+1):\n",
    "            layer_input = layer_outputs[-1] @ weights[i]\n",
    "            layer_output = sigmoid(layer_input)\n",
    "            layer_outputs.append(layer_output)\n",
    "\n",
    "        # Cálculo del error y la derivada de la función de activación en la capa de salida\n",
    "        error = y - layer_outputs[-1]\n",
    "        delta = error * sigmoid_derivative(layer_outputs[-1])\n",
    "\n",
    "        # Propagación hacia atrás\n",
    "        for i in range(hidden_layers-1, -1, -1):\n",
    "            delta = delta @ weights[i+1].T * sigmoid_derivative(layer_outputs[i+1])\n",
    "            weights[i] += layer_outputs[i].T @ delta * lr\n",
    "\n",
    "\n",
    "    # Predicción\n",
    "    layer_outputs = [X]\n",
    "    for i in range(hidden_layers+1):\n",
    "        layer_input = layer_outputs[-1] @ weights[i]\n",
    "        layer_output = sigmoid(layer_input)\n",
    "        layer_outputs.append(layer_output)\n",
    "    y_pred = np.where(layer_outputs[-1] >= 0.5, 1, -1)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "#Error del 39%\n",
    "#hidden_layers = 5\n",
    "#hidden_units = [24,22,16,8,4]\n",
    "hidden_layers = 3\n",
    "hidden_units = [4,3,2]\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 2\n",
    "\n",
    "    \n",
    "y_pred = perceptron_multicapa(X_ent, Y_ent, hidden_layers, hidden_units, lr, epochs)\n",
    "suma = 0\n",
    "for i in range (D_entre):\n",
    "        if(y_pred[i] != Y_ent[i]):\n",
    "            suma = suma + 1;\n",
    "            \n",
    "print(\"Y_pred\", y_pred)\n",
    "print(\"Y_ent\", Y_ent)\n",
    "print(\"Error: \", (suma*100)/(D_entre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763cf1c",
   "metadata": {},
   "source": [
    "Para hacer que el número de capas ocultas y de neuronas sean variables en este código, se pueden hacer \n",
    "los siguientes cambios:\n",
    "\n",
    "    \n",
    "En lugar de tener una única matriz de pesos para la capa oculta, se pueden crear n_hidden matrices de pesos,\n",
    "una para cada capa oculta.\n",
    "\n",
    "También se necesitará una matriz de pesos para la capa de salida, como antes.\n",
    "\n",
    "Se puede utilizar un bucle for para propagar hacia adelante y hacia atrás la información a través de\n",
    "todas las capas ocultas.\n",
    "\n",
    "Para actualizar los pesos, se deben utilizar las derivadas de la función de activación para cada capa \n",
    "oculta y de salida.\n",
    "\n",
    "Aquí está el código actualizado que implementa estas modificaciones:\n",
    "\n",
    "\n",
    "def perceptron_multicapa(X, y, n_hidden_layers, n_neurons_per_layer, eta, epochs, random_seed):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Inicializar pesos aleatorios para todas las capas ocultas y de salida\n",
    "    w_hidden = [np.random.randn(X.shape[1], n_neurons_per_layer)]\n",
    "    for i in range(n_hidden_layers-1):\n",
    "        w_hidden.append(np.random.randn(n_neurons_per_layer, n_neurons_per_layer))\n",
    "    w_output = np.random.randn(n_neurons_per_layer)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Propagación hacia adelante\n",
    "        hidden_layer_outputs = [sigmoid(np.dot(X, w_hidden[0]))]\n",
    "        for i in range(1, n_hidden_layers):\n",
    "            hidden_layer_input = np.dot(hidden_layer_outputs[i-1], w_hidden[i])\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "            hidden_layer_outputs.append(hidden_layer_output)\n",
    "        output_layer_input = np.dot(hidden_layer_outputs[-1], w_output)\n",
    "        y_pred = sigmoid(output_layer_input)\n",
    "\n",
    "        # Cálculo del error\n",
    "        error = y - y_pred\n",
    "\n",
    "        # Retropropagación del error\n",
    "        output_delta = error * y_pred * (1 - y_pred)\n",
    "        hidden_deltas = [output_delta.dot(w_output.T) * hidden_layer_outputs[-1] * (1 - hidden_layer_outputs[-1])]\n",
    "        for i in range(n_hidden_layers-2, -1, -1):\n",
    "            hidden_delta = hidden_deltas[0].dot(w_hidden[i+1].T) * hidden_layer_outputs[i] * (1 - hidden_layer_outputs[i])\n",
    "            hidden_deltas.insert(0, hidden_delta)\n",
    "\n",
    "        # Actualización de pesos\n",
    "        w_output += eta * hidden_layer_outputs[-1].T.dot(output_delta)\n",
    "        w_hidden[0] += eta * X.T.dot(hidden_deltas[0])\n",
    "        for i in range(1, n_hidden_layers):\n",
    "            w_hidden[i] += eta * hidden_layer_outputs[i-1].T.dot(hidden_deltas[i])\n",
    "\n",
    "    return w_hidden, w_output\n",
    "En este nuevo código, los argumentos n_hidden_layers y n_neurons_per_layer controlan \n",
    "el número de capas ocultas y el número de neuronas en cada capa oculta, respectivamente. \n",
    "El código utiliza una lista de matrices de pesos para almacenar los pesos de cada capa oculta.\n",
    "La propagación hacia adelante y hacia atrás de la información se realiza mediante un bucle for \n",
    "que recorre todas las capas ocultas. La función de activación utilizada es la función sigmoide, \n",
    "que se define en otro lugar del código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0537e",
   "metadata": {},
   "source": [
    "\n",
    "def perceptron_multicapa(X, y, n_hidden_layers, n_neurons_per_layer, eta, epochs, random_seed):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Inicializar pesos aleatorios para todas las capas ocultas y de salida\n",
    "    w_hidden = [np.random.randn(X.shape[1] + 1, n_neurons_per_layer)] # Agregar 1 para incluir b0\n",
    "    for i in range(n_hidden_layers-1):\n",
    "        w_hidden.append(np.random.randn(n_neurons_per_layer + 1, n_neurons_per_layer)) # Agregar 1 para incluir b0\n",
    "    w_output = np.random.randn(n_neurons_per_layer + 1) # Agregar 1 para incluir b0\n",
    "\n",
    "    # Agregar término independiente a la matriz de entrada X\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Propagación hacia adelante\n",
    "        hidden_layer_outputs = [sigmoid(np.dot(X, w_hidden[0]))]\n",
    "        for i in range(1, n_hidden_layers):\n",
    "            hidden_layer_input = np.dot(hidden_layer_outputs[i-1], w_hidden[i][:, :-1]) + w_hidden[i][:, -1] # Agregar término independiente b0\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "            hidden_layer_outputs.append(hidden_layer_output)\n",
    "        output_layer_input = np.dot(hidden_layer_outputs[-1], w_output[:-1]) + w_output[-1] # Agregar término independiente b0\n",
    "        y_pred = sigmoid(output_layer_input)\n",
    "\n",
    "        # Cálculo del error\n",
    "        error = y - y_pred\n",
    "\n",
    "        # Retropropagación del error\n",
    "        output_delta = error * y_pred * (1 - y_pred)\n",
    "        hidden_deltas = [output_delta.dot(w_output[:-1].T) * hidden_layer_outputs[-1] * (1 - hidden_layer_outputs[-1])]\n",
    "        for i in range(n_hidden_layers-2, -1, -1):\n",
    "            hidden_delta = hidden_deltas[0].dot(w_hidden[i+1][:, :-1].T) * hidden_layer_outputs[i] * (1 - hidden_layer_outputs[i])\n",
    "            hidden_deltas.insert(0, hidden_delta)\n",
    "\n",
    "        # Actualización de pesos\n",
    "        w_output[:-1] += eta * hidden_layer_outputs[-1].T.dot(output_delta)\n",
    "        w_output[-1] += eta * output_delta.sum(axis=0) # Actualizar b0 de la capa de salida\n",
    "        w_hidden[0][:, :-1] += eta * X.T.dot(hidden_deltas[0])\n",
    "        w_hidden[0][:, -1] += eta * hidden_deltas[0].sum(axis=0) # Actualizar b0 de la primera capa oculta\n",
    "        for i in range(1, n_hidden_layers):\n",
    "            w_hidden[i][:, :-1] += eta * hidden_layer_outputs[i-1].T.dot(hidden_deltas[i])\n",
    "            w_hidden[i][:, -1] += eta * hidden_deltas[i].sum(axis=0) # Actualizar b0 de la capa oculta i\n",
    "\n",
    "    return w_hidden, w_output\n",
    "En este código, se agregó una columna de valores 1 a la matriz de entrada X para incluir el término independiente b0. Además, se agregó el término independiente b0 en todas las capas ocultas y de salida. Para hacer esto, se agregó una columna extra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
